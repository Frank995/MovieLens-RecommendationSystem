---
title: "HarvardX PH125.9x Data Science Capstone Movielens Project"
author: "Francesco Pudda"
date: "27/3/2020"
output: pdf_document
bibliography: MovieLens-RecommendationSystem-Bibliography.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Abstract

Nowadays more and more streaming services are springing up, and users often find themselves in often find themselves in a situation where they don't know what to choose among all the possibilities. Luckily, such services also provide the users with suggestions concerning recommended films or series they could probably enjoy based on their viewing history.
Such feature is so important to push Netflix in 2009 to organise a 1M$ competition to bestow to the group who had trained the best recommandation model [@Netflix].
In this report I am going to show how I achieved an efficient reccomandation system somehow based also on their work.

# Introduction

The data used in this work was generated by GroupLens [@Dataset] and contains 10M films ratings arranged by user, film, genre and review date. Dataset is pretty huge and it was necessary to perform some optimisation, like free memory whenever possible and avoid operations that could occupy the whole RAM and SWAP and freeze the computer.

I first inspected the data frame data classes and then proceeded to study general trends of data from different points of view. Later I tried some simple learning approaches like linear and logistic regression using the caret package but unfortunately computational time and needed memory for calling `lm()` with such a huge dataset is prohibitive.
This forced me to try different algorithms that brought me to partially base my work upon the paper of the winners of Netflix Grand Prize [@Algorithm].

Here you can see libraries that I have used in this work:
```{r requirements, include=FALSE}
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")
if(!require(Metrics)) install.packages("Metrics", repos = "http://cran.us.r-project.org")
if(!require(knitr)) install.packages("knitr", repos = "http://cran.us.r-project.org")
if(!require(kableExtra)) install.packages("kableExtra", repos = "http://cran.us.r-project.org")
```
```{r libraries, message=FALSE, warning=FALSE}
library(tidyverse)
library(caret)
library(data.table)
library(lubridate)
library(Metrics)
library(knitr)
library(kableExtra)
```
```{r load, include=FALSE}
dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
                                           title = as.character(title),
                                           genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data.
set.seed(1, sample.kind="Rounding")
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set.
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set.
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)

# Free RAM
invisible(gc())
```

# Data inspection and exploratory analysis

A first look at the data reveals that dataset columns are so defined:

```{r intro}
head(edx, 3)
sapply(edx, class)
```

Before starting any analysis I carried out a quick optimisation step to reduce memory usage and possibly computational demand on CPU. Noted that _movieId_ was stored as numeric, first thing to do was to convert it to integer, class that needs half the memory of the other one. Then, since _title_ column was useless as far as data analysis and visualisation were concerned, it was deleted too. Then, rows in _edx_ whose userId or movieId entries were not in _validation_ were deleted because it was pointless to train for those. And finally, since I am needing them later, I created beforehands two columns from timestamp: one for the week of the year and the other for the date (rounded to the closest week) when the reviews were made (timestamp was then deleted). The reason for this was to avoid to mutate the data set everytime I needed those columns.

```{r optimisation}
# Convert movieId
edx$movieId <- as.integer(edx$movieId)

# Remove title and some row
validation <- validation %>% 
  select(-title)
edx <- edx %>% select(-title) %>%
  semi_join(validation, by = "userId") %>%
  semi_join(validation, by = "movieId")

# Create two columns from timestamp and remove it
edx <- edx %>% 
  mutate(time = round_date(as_datetime(timestamp), unit = "week"),
         week = week(as_datetime(timestamp))) %>%
  select(-timestamp)
validation <- validation %>%
  mutate(time = round_date(as_datetime(timestamp), unit = "week"),
         week = week(as_datetime(timestamp))) %>%
  select(-timestamp)
```

Exploratory analysis was carried out by grouping the data frame by different variables and inspecting their trends to discover possible patterns or biases. It is in fact reasonable to think that certain films will have better average score, likewise different users may have different preferences or some genres might be averagely more appreciated by the audience. This is proven by looking at the histograms below.

```{r distributions}
# Ratings per film
edx %>% 
  group_by(movieId) %>%
  summarise(count = n()) %>%
  ggplot(aes(count)) + 
  geom_histogram(bins = 30, color = "blue") +
  scale_x_log10() + 
  ggtitle("Number of ratings per film") +
  xlab("Film ID") + 
  ylab("Number of ratings")

# Ratings per user
edx %>% 
  group_by(userId) %>%
  summarise(count = n()) %>%
  ggplot(aes(count)) + 
  geom_histogram(bins = 30, color = "red") +
  scale_x_log10() + 
  ggtitle("Number of ratings per user") +
  xlab("User ID") + 
  ylab("Number of ratings")

# Ratings per genre
edx %>% 
  group_by(genres) %>%
  summarise(count = n()) %>%
  ggplot(aes(count)) + 
  geom_histogram(bins = 30, color = "green") +
  scale_x_log10() + 
  ggtitle("Number of ratings per genre") +
  xlab("Genre") + 
  ylab("Number of ratings") +
  theme(axis.text.x=element_blank(),
        axis.ticks.x=element_blank())
```

It is clear that ratings greatly depends on the film, user and genre. Let's now consider the influence time may have over ratings.

```{r time_effect, message=FALSE, warning=FALSE}
# Compute the time regression
regression <- edx %>%
  mutate(timestamp = as.numeric(time)) %>%
  lm(rating ~ timestamp, data = .)

# Plotting the time series.
cols <- c("Smooth interpolation"="#0000ff","Regression line"="#ff0000")
edx %>%
  group_by(time) %>%
  summarise(avg = mean(rating)) %>%
  ggplot(aes(time, avg)) +
  geom_point(size=1.1) +
  geom_smooth(aes(colour = "Smooth interpolation"), size = 1.2) +
  geom_abline(aes(colour = "Regression line",
                  intercept = regression$coefficients["(Intercept)"],
                  slope = regression$coefficients["timestamp"]),
              size = 1.3) +
  scale_colour_manual(name="Legend", values=cols) +
  ggtitle("Average rating per week") + 
  xlab("Year") + 
  ylab("Average rating")

# Intra year effect
edx %>%
  group_by(week) %>%
  summarise(avg = mean(rating)) %>%
  ggplot(aes(week, avg)) +
  geom_point() +
  geom_smooth() +
  ggtitle("Average rating throughout the year") + 
  xlab("Week of the year") + 
  ylab("Average rating")
```

In this plot the blue line is the interpolated graph showing also confidence intervals in gray. In red it is plotted the regression line. According to this plot there is a clear downwards trend even though the slope is very small. Despite this, in 1995 and early 1996 there were many weeks when average ratings were reported to be much higher than any other week in the following years making them anomalous years.

In addition, it can be also noted a sort of pattern during the average year. Average ratings tend, in fact, to fall off around spring-summer, and to rise up during autumn-winter. In particular it can be clearly seen an increase towards the end of the year.

After this, data were again grouped by different variables to check for any other information. In other words, I tried to get more insights and visual clues about the biases discussed above.

```{r exploratory, message=FALSE, warning=FALSE}
# These pieces of code below will study other
# biases by plotting the number of ratings
# vs. average ratings calculated by grouping by
# userId and movieId.
edx %>%
  group_by(movieId) %>%
  summarise(count = n(), avg = mean(rating)) %>%
  ggplot(aes(count, avg)) +
  geom_point() +
  geom_smooth() +
  ggtitle("Average ratings based on films number of reviews") + 
  xlab("Reviews count") + 
  ylab("Average rating")

edx %>%
  group_by(userId) %>% summarise(count = n(), avg = mean(rating)) %>%
  ggplot(aes(count, avg)) +
  geom_point() +
  geom_smooth() +
  ggtitle("Average ratings based on users' number of reviews") + 
  xlab("Reviews count") + 
  ylab("Average rating")

# To conclude this is to check any possible
# genre effect, firstly with 'compound genres'
# and secondly by considering 'only single' ones.
# By compound genres I mean genres as given in the
# dataset, e.g Drama|War|Action.
edx %>%
  group_by(genres) %>%
  summarise(n = n(), avg = mean(rating), se = sd(rating)/sqrt(n())) %>%
  filter(n >= quantile(n, 0.98)) %>% 
  mutate(genres = reorder(genres, avg)) %>%
  ggplot(aes(x = genres, y = avg, ymin = avg - 2*se, ymax = avg + 2*se)) + 
  geom_point() +
  geom_errorbar() + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ggtitle("Average ratings by compound genre") + 
  xlab("Genres") + 
  ylab("Average rating")

# Splitting by and then grouping by genres would
# would result in a computer freeze because
# the data frame would become too huge.
# For this reason in this case I am first 
# grouping by movieId.
edx %>%
  group_by(movieId) %>%
  summarise(count = n(), avg = mean(rating), genres = first(genres)) %>%
  separate_rows(genres, sep="\\|") %>%
  group_by(genres) %>%
  summarise(avg = mean(avg)) %>%
  mutate(genres = reorder(genres, avg)) %>%
  ggplot(aes(x = genres, y = avg)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ggtitle("Average ratings by genre") + 
  xlab("Genre") + 
  ylab("Average rating")
```

We can note an upwards trend in average ratings against number of reviews. A reasonable interpretation of this effect could lie in the fact that most watched and popular films are likely considered _better_ by the most and thus reviewed more.

Turning to user effect, the plot shows a quick fall and stabilisation around the average as the number of reviews grows. A reason for this could be that users with so many reviews may be more neutral and _unbiased_ whereas, on the other hand, users with less reviews could have reviewed only films they really liked or disliked.

To conclude, also genres have some significant influence on average ratings both in case of _compound_ genres, both in case of _single_ ones. Probably because some of them are just more enjoyed by the population.

```{r, include=FALSE}
rm(regression, cols)
invisible(gc())
```

# Training models

### Bias models

To train the model I used a simple linear regression algorithm as the simplest recommendation system possible. I started by considering the most trivial approach: same average rating for every user and every film, with differences only given by randomness ($\epsilon$).
$$ Y = \mu + \epsilon$$
Where $\mu$ is the average rating for the films. The previous exploratory analysis though, provided us with insights concerning possible biases in the data that could be useful for improve such a naive model.

*Film effect*: as shown, some films can be just preferred by the population and thus rated better than average. This can be modelles by a vector taking into account the effect a film causes on ratings.
$$ Y_{i} = \mu + b_i + \epsilon_{i} $$
Where $b_i$ is the film bias vector.

*User effect*: as above, some users may just tend to give higher ratings to movies. As above, we can model such effect with another vector.
$$ Y_{i,u} = \mu + b_i + b_u + \epsilon_{i,u} $$
Where $b_u$ is the user bias.

*Time and week effect*: again, as shown in the exploratory phase, date has some influence in how films are rated. Here I considered two different time effect: the first one concerns the trend of ratings over an average year, whereas the second one ragards the evolution of them across the years.
$$ Y_{i,u} = \mu + b_i + b_u + f(d_{u,i}) + \epsilon_{i,u} $$
Here, $f$ is a smooth time function and it is described in detail in @Algorithm. For this study, I applied an easier approach, calculating such terms in the same way as the user and film effect. Differently from the paper I also considered the intra-year effect.
$$ Y_{i,u,w,t} = \mu + b_i + b_u + b_w + b_t + \epsilon_{i,u,w,t} $$

*Genre effect*: finally, since it was proven that some genres have significant higher ratings it will be important to model their effect, too. Again I will be using the simplest linear model possible, exactly like the previous terms.
$$ Y_{i,u,w,t,g} = \mu + b_i + b_u + b_w + b_t + b_g + \epsilon_{i,u,w,t,g} $$
In this regard I tried to calculate effects generated both by _compound_ genres both by considering the _single_ genres by themselves. Unfortunately, in this last case I had to stop the computation since it was requiring too much time and memory and therefore it will not be included here.

Once mathematical equations were laid out I began to estimate each of the terms:

* $\hat{\mu}$ was estimated by simply averaging all the rating

* $\hat{b_i}$ was calculated as the average of $\hat{Y} - \hat{\mu}$

* $\hat{b_u}$ as the average of $\hat{Y_{i}} - \hat{\mu} - \hat{b_i}$

* $\hat{b_w}$ as the average of $\widehat{Y_{i,u}} - \hat{\mu} - \hat{b_i} - \hat{b_u}$

* $\hat{b_t}$ as the average of $\widehat{Y_{i,u,w}} - \hat{\mu} - \hat{b_i} - \hat{b_u} - \hat{b_w}$

* $\hat{b_g}$ as the average of $\widehat{Y_{i,u,w,t}} - \hat{\mu} - \hat{b_i} - \hat{b_u} - \hat{b_w} - \hat{b_t}$

* And finally predicted ratings were calculated as $\widehat{Y_{i,u,w,t,g}} = \hat{\mu} + \hat{b_i} + \hat{b_u} + \hat{b_w} + \hat{b_t} + \hat{b_g}$

### Regularisation
Let us bring the model one step forward. We can note from the first histograms, that there are many users, films and genres with really few reviews, and this could lead to over- or underestimate ratings due to small sample size. Regression permits us to penalise such estimates by adding a penalty to $b_i$, $b_u$ or $b_g$ when the sample size is small.

This can be formulated in mathematical terms:
$$ b_i(\lambda) = \frac{1}{\lambda + n_i} \sum_{i=0}^{n_i} {(\hat{Y} - \hat{\mu})} $$
$$ b_u(\lambda) = \frac{1}{\lambda + n_u} \sum_{u=0}^{n_u} {(\hat{Y} - \hat{\mu} - \hat{b_i})} $$
$$ b_g(\lambda) = \frac{1}{\lambda + n_g} \sum_{g=0}^{n_g} {(\hat{Y} - \hat{\mu} - \hat{b_i}  - \hat{b_u}  - \hat{b_w}  - \hat{b_t})} $$
When $n$ is very large $\lambda$ is negligible and such formula will converge to the arithmetic mean as $n$ increases. For small $n$ and large $\lambda$ the fraction, and so the estimate, will converge to $0$, though, penalising thus small size samples.

Best $\lambda$ is unknown so a trial-and-error approach had to be followed to find the best parameter.

# Results

Before moving forward it is important to state the metric by the which the model accuracy will be being assessed. Since ratings are numeric value and the aim is to predict them, a suitable metric is the root-mean-square error (or RMSE) that is so defined:
$$ RMSE = \sqrt{\frac{1}{N} \sum \left( \hat{y} - y \right)^2 } $$
Where $\hat{y}$ is the predicted value and $y$ the real one.

During model checking bootstraps cross-validation was used with five subsets so that four of them were used for training and one for testing. Validation set was used only at the very end to test the final model. Five bootstraps were chosen as trade-off between precision and computational demand.
```{r bootstrap}
n_bootstraps <- 5
bs_index <- createDataPartition(edx$rating, times=n_bootstraps, p=0.1, list=FALSE)
```

```{r bias1}
# No bias, just average
mu <- lapply(1:n_bootstraps, function(i){
  mean(edx[-bs_index[,i], "rating"])
})

naive_rmse <- mean(sapply(1:n_bootstraps, function(i){
  y_hat <- mu[[i]]
  rmse(edx[bs_index[,i],"rating"], y_hat)
}))

cat("Naive rmse: ", naive_rmse, "\n")

# Movie bias
movie_bias <- lapply(1:n_bootstraps, function(i){
  edx[-bs_index[,i],] %>%
  group_by(movieId) %>% 
  summarise(b_i = mean(rating - mu[[i]]))
})

movie_rmse <- mean(sapply(1:n_bootstraps, function(i){
  y_hat <- mu[[i]] + edx[bs_index[,i],] %>%
    group_by(movieId) %>%
    left_join(movie_bias[[i]], by = "movieId") %>%
    pull(b_i)
  nas <- which(is.na(y_hat))
  rmse(edx[bs_index[,i],"rating"][-nas], y_hat[-nas])
}))

cat("Movie effect rmse: ", movie_rmse, "\n")

# User bias
user_bias <- lapply(1:n_bootstraps, function(i){
  edx[-bs_index[,i],] %>%
    left_join(movie_bias[[i]], by='movieId') %>%
    group_by(userId) %>% 
    summarise(b_u = mean(rating - mu[[i]] - b_i))
})

user_rmse <- mean(sapply(1:n_bootstraps, function(i){
  y_hat <- mu[[i]] + edx[bs_index[,i],] %>%
    group_by(movieId) %>%
		left_join(movie_bias[[i]], by = "movieId") %>%
		left_join(user_bias[[i]], by = "userId") %>%
		ungroup() %>%
		select(b_i, b_u) %>%
    rowSums()
  nas <- which(is.na(y_hat))
	rmse(edx[bs_index[,i],"rating"][-nas], y_hat[-nas])
}))

cat("Movie + user effect rmse: ", user_rmse, "\n")
```

Let us move on to the time and year effects described in the previous section, to check which one (or if both) have a positive impact on predictions.

```{r bias2}
# Week of the year effect
week_bias <- lapply(1:n_bootstraps, function(i){
  edx[-bs_index[,i],] %>%
    left_join(movie_bias[[i]], by='movieId') %>%
	  left_join(user_bias[[i]], by='userId') %>%
    group_by(week) %>% 
    summarise(b_w = mean(rating - mu[[i]] - b_i - b_u))
})

week_rmse <- mean(sapply(1:n_bootstraps, function(i){
  y_hat <- mu[[i]] + edx[bs_index[,i],] %>%
    group_by(movieId) %>%
    left_join(movie_bias[[i]], by = "movieId") %>%
		left_join(user_bias[[i]], by = "userId") %>%
		left_join(week_bias[[i]], by = "week") %>%
		ungroup() %>%
		select(b_i, b_u, b_w) %>% 
		rowSums()
	nas <- which(is.na(y_hat))
	rmse(edx[bs_index[,i],"rating"][-nas], y_hat[-nas])
}))

cat("Movie + user + week effect rmse: ", week_rmse, "\n")

# Time effect
time_bias <- lapply(1:n_bootstraps, function(i){
  edx[-bs_index[,i],] %>%
    left_join(movie_bias[[i]], by='movieId') %>%
	  left_join(user_bias[[i]], by='userId') %>%
	  left_join(week_bias[[i]], by='week') %>%
    group_by(time) %>% 
    summarise(b_t = mean(rating - mu[[i]] - b_i - b_u - b_w))
})

time_rmse <- mean(sapply(1:n_bootstraps, function(i){
  y_hat <- mu[[i]] + edx[bs_index[,i],] %>%
    group_by(movieId) %>%
		left_join(movie_bias[[i]], by = "movieId") %>%
		left_join(user_bias[[i]], by = "userId") %>%
		left_join(week_bias[[i]], by = "week") %>%
		left_join(time_bias[[i]], by = "time") %>%
		ungroup() %>%
		select(b_i, b_u, b_w, b_t) %>% 
		rowSums()
	nas <- which(is.na(y_hat))
	rmse(edx[bs_index[,i],"rating"][-nas], y_hat[-nas])
}))

cat("Movie + user + time effect rmse: ", time_rmse, "\n")
```

They both improve predictions even though not as much as the first two biases. Let's finally check the genre effect.

```{r bias3}
# Genre effect
genre_bias <- lapply(1:n_bootstraps, function(i){
  edx[-bs_index[,i],] %>%
    left_join(movie_bias[[i]], by='movieId') %>%
	  left_join(user_bias[[i]], by='userId') %>%
	  left_join(week_bias[[i]], by='week') %>%
	  left_join(time_bias[[i]], by='time') %>%
    group_by(genres) %>% 
    summarise(b_g = mean(rating - mu[[i]] - b_i - b_u - b_w - b_t))
})

genre_rmse <- mean(sapply(1:n_bootstraps, function(i){
  y_hat <- mu[[i]] + edx[bs_index[,i],] %>%
    group_by(movieId) %>%
		left_join(movie_bias[[i]], by = "movieId") %>%
		left_join(user_bias[[i]], by = "userId") %>%
		left_join(week_bias[[i]], by = "week") %>%
		left_join(time_bias[[i]], by = "time") %>%
		left_join(genre_bias[[i]], by = "genres") %>%
		ungroup() %>%
		select(b_i, b_u, b_w, b_t, b_g) %>% 
		rowSums()
	nas <- which(is.na(y_hat))
	rmse(edx[bs_index[,i],"rating"][-nas], y_hat[-nas])
}))

cat("Movie + user + time + genre effect rmse: ", genre_rmse, "\n")
```
```{r, include=FALSE}
# Clean up
rm(mu, movie_bias, user_bias, week_bias, time_bias, genre_bias)
invisible(gc())
```

Results are pretty good but, as stated above, they could be further improved via regularisation. I so define a vector of values that will be used as regularisation parameters and rmse will be stored. Best one will be used in the final validation step for building up the model.

```{r regularisation}
# Possible lambdas
# NOTE: At first I used with a wider range but
# computational time was very high. So, after having
# known the best parameter I reduced the interval
# to make the script run faster for the grader and
# peers
lambdas <- seq(4, 7, 0.5)

# Try each lambda and get the resulting rmse
# NOTE: it will take several minutes
rmse_lambda <- sapply(lambdas, function(lambda){
  mu <- lapply(1:n_bootstraps, function(i){
    mean(edx[-bs_index[,i], "rating"])
  })
  
  movie_bias <- lapply(1:n_bootstraps, function(i){
    edx[-bs_index[,i],] %>%
      group_by(movieId) %>% 
      summarise(b_i = sum(rating - mu[[i]]) / (lambda + n()))
  })
  
  user_bias <- lapply(1:n_bootstraps, function(i){
    edx[-bs_index[,i],] %>%
      left_join(movie_bias[[i]], by='movieId') %>%
      group_by(userId) %>% 
      summarise(b_u = sum(rating - mu[[i]] - b_i) / (lambda + n()))
  })
  
  week_bias <- lapply(1:n_bootstraps, function(i){
    edx[-bs_index[,i],] %>%
      left_join(movie_bias[[i]], by='movieId') %>%
      left_join(user_bias[[i]], by='userId') %>%
      group_by(week) %>% 
      summarise(b_w = mean(rating - mu[[i]] - b_i - b_u))
  })
  
  time_bias <- lapply(1:n_bootstraps, function(i){
    edx[-bs_index[,i],] %>%
      left_join(movie_bias[[i]], by='movieId') %>%
      left_join(user_bias[[i]], by='userId') %>%
      left_join(week_bias[[i]], by='week') %>%
      group_by(time) %>% 
      summarise(b_t = mean(rating - mu[[i]] - b_i - b_u - b_w))
  })
  
  genre_bias <- lapply(1:n_bootstraps, function(i){
    edx[-bs_index[,i],] %>%
      left_join(movie_bias[[i]], by='movieId') %>%
      left_join(user_bias[[i]], by='userId') %>%
      left_join(week_bias[[i]], by='week') %>%
      left_join(time_bias[[i]], by='time') %>%
      group_by(genres) %>% 
      summarise(b_g = sum(rating - mu[[i]] - b_i - b_u - b_w - b_t) / (lambda + n()))
  })
  
  mean(sapply(1:n_bootstraps, function(i){
    y_hat <- mu[[i]] + edx[bs_index[,i],] %>%
      group_by(movieId) %>%
      left_join(movie_bias[[i]], by = "movieId") %>%
      left_join(user_bias[[i]], by = "userId") %>%
      left_join(week_bias[[i]], by = "week") %>%
      left_join(time_bias[[i]], by = "time") %>%
      left_join(genre_bias[[i]], by = "genres") %>%
      ungroup() %>%
      select(b_i, b_u, b_w, b_t, b_g) %>% 
      rowSums()
    nas <- which(is.na(y_hat))
    rmse(edx[bs_index[,i],"rating"][-nas], y_hat[-nas])
  }))
})

# Plot obtained RMSEs
plot(lambdas, rmse_lambda, main="RMSE by regularisation parameter",
     xlab="Lambda", ylab="RMSE")

regularised_rmse <- min(rmse_lambda)
best_lambda <- lambdas[which.min(rmse_lambda)]

cat("Best regularised rmse: ", regularised_rmse, "\n")
cat("Best regularisation parameter: ", best_lambda, "\n")
```
```{r, include=FALSE}
# Clean up
rm(n_bootstraps, bs_index, rmse_lambda, lambdas)
invisible(gc())
```

Now it's possible to build the final model. Since we finished model checking and now we are building the model cross validation is not needed anymore and we can train the best model out of the whole dataset using the $\lambda$ obtained.

```{r finalmodel, echo = FALSE}
mu <- mean(edx$rating)

movie_bias <- edx %>%
  group_by(movieId) %>% 
  summarise(b_i = sum(rating - mu) / (best_lambda + n()))

user_bias <- edx %>%
  left_join(movie_bias, by='movieId') %>%
  group_by(userId) %>%
  summarise(b_u = sum(rating - mu - b_i) / (best_lambda + n()))

week_bias <- edx %>%
  left_join(movie_bias, by='movieId') %>%
  left_join(user_bias, by='userId') %>%
  group_by(week) %>% 
  summarise(b_w = mean(rating - mu - b_i - b_u))

time_bias <- edx %>%
  left_join(movie_bias, by='movieId') %>%
  left_join(user_bias, by='userId') %>%
  left_join(week_bias, by='week') %>%
  group_by(time) %>%
  summarise(b_t = mean(rating - mu - b_i - b_u - b_w))

genre_bias <- edx %>%
  left_join(movie_bias, by='movieId') %>%
  left_join(user_bias, by='userId') %>%
  left_join(week_bias, by='week') %>%
  left_join(time_bias, by='time') %>%
  group_by(genres) %>% 
  summarise(b_g = sum(rating - mu - b_i - b_u - b_w - b_t) / (best_lambda + n()))

y_hat <- mu + validation %>%
  group_by(movieId) %>%
  left_join(movie_bias, by = "movieId") %>%
  left_join(user_bias, by = "userId") %>%
  left_join(week_bias, by = "week") %>%
  left_join(time_bias, by = "time") %>%
  left_join(genre_bias, by = "genres") %>%
  ungroup() %>%
  select(b_i, b_u, b_w, b_t, b_g) %>% 
  rowSums()

final_rmse <- rmse(validation$rating, y_hat)

cat("Final rmse: ", final_rmse, "\n")
```
```{r, include=FALSE}
# Clean up everything
rm(mu, movie_bias, user_bias, week_bias, time_bias, genre_bias,
   best_lambda, y_hat, bs_index, edx, validation, lambdas, n_bootstraps)
invisible(gc())
```

# Conclusion and future work

This project has analysed the MovieLens 10M Dataset with the aim of extrapolating data patterns or biases that could be useful for training a recommendation system. In order to do this, the original dataset was split into a training set, called _edx_, and a test set, called _validation_, with a 9:1 ratio. The most important thing to be sure was that the _validation_ set had to be used only for the final performance calculation, and to avoid any overtraining due to the use of such set in the training phase.

First step was the exploratory analysis, crucial to get visual clues about data distributions and patterns that were later used to train the recommandation system. Such system took into account different biases: movie, user, time and genre effect. So as to further improve the final model, regularisation was used to penalise predictions resulted by small sample size, that could be over- or underestimated. During the model checking phase bootstrap cross-validation was used to estimate models performances.

```{r summary, echo=FALSE, results="asis"}
# Store results in a data frame
results <- data.frame(Method = c("Naive", "+ movie effect",
                                 "+ user effer",
                                 "+ week effect",
                                 "+ time effect",
                                 "+ genre effect",
                                 "+ regularisation",
                                 "Final model (validation)"),
                      RMSE = c(naive_rmse, movie_rmse, user_rmse, week_rmse,
                               time_rmse, genre_rmse, regularised_rmse,
                               final_rmse))

results %>% kable() %>% kable_styling() %>% row_spec(8, bold = TRUE, color = "white", background = "#D7261E")
```

An important note that must be made is that such simple, but pretty accurate, approach was the only one that could be tried out by using a common, low-mid tier, hardware. In fact, any attempt to use other _pre-defined_ function (e.g. `lm()`) failed due to the huge memory requirements. In this regard, further improvement could be made by attempting to run different training models via cloud computing. In this way it would be possible to run a linear or generalised linear regression, or other methods like random forest regression to try to obtain better estimates.

# References